{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30887,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SFT on Reasoning Dataset for Qwen 2.5 0.5B"
      ],
      "metadata": {
        "id": "DjCSWldyDWYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U tensorboardX\n",
        "!pip install -q wandb\n",
        "!pip install -1 -U datasets"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:56:31.172547Z",
          "iopub.execute_input": "2025-03-05T09:56:31.172887Z",
          "iopub.status.idle": "2025-03-05T09:56:52.738235Z",
          "shell.execute_reply.started": "2025-03-05T09:56:31.172861Z",
          "shell.execute_reply": "2025-03-05T09:56:52.737410Z"
        },
        "id": "0MTnNs4aDWYg",
        "outputId": "c92401e9-43fa-40f2-dbfc-55a68275d06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\nUsage:   \n  pip3 install [options] <requirement specifier> [package-index-options] ...\n  pip3 install [options] -r <requirements file> [package-index-options] ...\n  pip3 install [options] [-e] <vcs project url> ...\n  pip3 install [options] [-e] <local project path> ...\n  pip3 install [options] <archive url/path> ...\n\nno such option: -1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, TaskType\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:57:12.944182Z",
          "iopub.execute_input": "2025-03-05T09:57:12.944479Z",
          "iopub.status.idle": "2025-03-05T09:57:36.932753Z",
          "shell.execute_reply.started": "2025-03-05T09:57:12.944454Z",
          "shell.execute_reply": "2025-03-05T09:57:36.931878Z"
        },
        "id": "uyg9NaagDWYh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\",'v0')\n",
        "# think_prompt = \"\"\"You are a reasoning AI model solving complex problems by breaking into simple reasoning steps.before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\"\"\"\n",
        "def format_example(example):\n",
        "    \"\"\"Formats dataset example into a prompt-response format.\"\"\"\n",
        "    return {\"text\":f\"Problem: {example['problem']}\\nReasoning: {example['reannotated_assistant_content']}\\nSolution: {example['solution']}\"}\n",
        "\n",
        "# dataset = dataset.map(lambda x: {\"text\": format_example(x)})\n",
        "dataset = dataset.map(format_example,remove_columns = ['id', 'reannotated_assistant_content', 'problem', 'source', 'solution', 'verified', 'quality_metrics'])\n",
        "dataset\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:57:36.933931Z",
          "iopub.execute_input": "2025-03-05T09:57:36.934182Z",
          "iopub.status.idle": "2025-03-05T09:58:05.039679Z",
          "shell.execute_reply.started": "2025-03-05T09:57:36.934160Z",
          "shell.execute_reply": "2025-03-05T09:58:05.038989Z"
        },
        "colab": {
          "referenced_widgets": [
            "64782ab1ccfd4a46b15ec0046c649b6d",
            "74700bbd5f444832b8a3848559379379",
            "14d8dca140044b02a17af3785c58ac60",
            "04f4edec3ff94bd7a43e78cfc61a0986",
            "516d17c5824041d8975acada12c2d9b0",
            "4019dabbab2d44b591247b5ef7386022"
          ]
        },
        "id": "iD8kEOGbDWYh",
        "outputId": "ec140ac6-7b50-44ce-f3e3-a31e44681f2d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/2.70k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64782ab1ccfd4a46b15ec0046c649b6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00003.parquet:   0%|          | 0.00/180M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74700bbd5f444832b8a3848559379379"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00001-of-00003.parquet:   0%|          | 0.00/187M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14d8dca140044b02a17af3785c58ac60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00002-of-00003.parquet:   0%|          | 0.00/188M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04f4edec3ff94bd7a43e78cfc61a0986"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/171647 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "516d17c5824041d8975acada12c2d9b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/171647 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4019dabbab2d44b591247b5ef7386022"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 171647\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].train_test_split(0.0001)\n",
        "dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:58:05.041415Z",
          "iopub.execute_input": "2025-03-05T09:58:05.041657Z",
          "iopub.status.idle": "2025-03-05T09:58:05.103155Z",
          "shell.execute_reply.started": "2025-03-05T09:58:05.041636Z",
          "shell.execute_reply": "2025-03-05T09:58:05.102317Z"
        },
        "id": "yWBysIJJDWYh",
        "outputId": "dba48d1f-ff42-44f1-bcb6-21cc914717a3"
      },
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 171629\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 18\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatmlSpecialTokens(str, Enum):\n",
        "    think = \"<think>\"\n",
        "    eothink = \"</think>\"\n",
        "    pad_token = \"<pad>\"\n",
        "    eos_token = \"<eos>\"\n",
        "    @classmethod\n",
        "    def list(cls):\n",
        "        return [c.value for c in cls]\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:58:05.104520Z",
          "iopub.execute_input": "2025-03-05T09:58:05.104848Z",
          "iopub.status.idle": "2025-03-05T09:58:10.273723Z",
          "shell.execute_reply.started": "2025-03-05T09:58:05.104825Z",
          "shell.execute_reply": "2025-03-05T09:58:10.272786Z"
        },
        "id": "O_elBSRhDWYh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Change to your base variantmodel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
        "        additional_special_tokens=ChatmlSpecialTokens.list()\n",
        "    )\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             attn_implementation='eager',\n",
        "                                             device_map=\"auto\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(torch.bfloat16)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:58:10.274769Z",
          "iopub.execute_input": "2025-03-05T09:58:10.275016Z",
          "iopub.status.idle": "2025-03-05T09:58:33.837784Z",
          "shell.execute_reply.started": "2025-03-05T09:58:10.274986Z",
          "shell.execute_reply": "2025-03-05T09:58:33.836892Z"
        },
        "colab": {
          "referenced_widgets": [
            "402b6fd59b5d4b25bdfb6b12212d8962",
            "fad90b2a87ca410898a3bbce484ae8a5",
            "d10c51b3ebbd436bb9e49181fcfe1c05",
            "e74af352220d4326b70b49402602830f",
            "ac37d41d65ff4f54827e6c6aef8c6bda",
            "d587e5a9591549cca48c9ec30d93ad44",
            "88affc15712a4493b417ec19c1394acb"
          ]
        },
        "id": "mBqrM5joDWYh",
        "outputId": "313fffcf-c17e-4263-d3b7-fbc5699ff9f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "402b6fd59b5d4b25bdfb6b12212d8962"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fad90b2a87ca410898a3bbce484ae8a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d10c51b3ebbd436bb9e49181fcfe1c05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e74af352220d4326b70b49402602830f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac37d41d65ff4f54827e6c6aef8c6bda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d587e5a9591549cca48c9ec30d93ad44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88affc15712a4493b417ec19c1394acb"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151669, 1536)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen2DecoderLayer(\n        (self_attn): Qwen2Attention(\n          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1536, out_features=151669, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "rank_dimension = 16\n",
        "lora_alpha = 64\n",
        "lora_dropout = 0.05\n",
        "\n",
        "peft_config = LoraConfig(r=rank_dimension,\n",
        "                         lora_alpha=lora_alpha,\n",
        "                         lora_dropout=lora_dropout,\n",
        "                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"], # which layer in the transformers do we target ?\n",
        "                         task_type=TaskType.CAUSAL_LM)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:58:33.838797Z",
          "iopub.execute_input": "2025-03-05T09:58:33.839110Z",
          "iopub.status.idle": "2025-03-05T09:58:33.843376Z",
          "shell.execute_reply.started": "2025-03-05T09:58:33.839078Z",
          "shell.execute_reply": "2025-03-05T09:58:33.842577Z"
        },
        "id": "dlCKEDx6DWYh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Apply LoRA for efficient training\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "# )\n",
        "# model = get_peft_model(model, lora_config)\n",
        "# model.print_trainable_parameters()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T05:34:04.566246Z",
          "iopub.execute_input": "2025-02-16T05:34:04.567010Z",
          "iopub.status.idle": "2025-02-16T05:34:04.898764Z",
          "shell.execute_reply.started": "2025-02-16T05:34:04.566976Z",
          "shell.execute_reply": "2025-02-16T05:34:04.897932Z"
        },
        "id": "biK2yismDWYh",
        "outputId": "94239873-1a41-41fc-a530-f312d07f32e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 1,843,200 || all params: 3,087,781,888 || trainable%: 0.0597\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:58:33.844212Z",
          "iopub.execute_input": "2025-03-05T09:58:33.844458Z",
          "iopub.status.idle": "2025-03-05T09:58:39.249873Z",
          "shell.execute_reply.started": "2025-03-05T09:58:33.844429Z",
          "shell.execute_reply": "2025-03-05T09:58:39.248760Z"
        },
        "colab": {
          "referenced_widgets": [
            "252e826b7d254e489830d0f279a70749"
          ]
        },
        "id": "g8bcKxPVDWYh",
        "outputId": "fdfd4453-fc92-4828-a208-06f7f318ed12"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "252e826b7d254e489830d0f279a70749"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:59:24.402330Z",
          "iopub.execute_input": "2025-03-05T09:59:24.402712Z",
          "iopub.status.idle": "2025-03-05T09:59:24.407121Z",
          "shell.execute_reply.started": "2025-03-05T09:59:24.402677Z",
          "shell.execute_reply": "2025-03-05T09:59:24.406511Z"
        },
        "id": "O97rhBBDDWYh",
        "outputId": "d8d76ea1-b0bd-488a-dd72-6d46a93b3801"
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda', index=0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "username=\"Navaneeth\"    # REPLCAE with your Hugging Face username\n",
        "output_dir = \"Qwen2.5-1.5B-thinking-reasoning-model-V0\" # The directory where the trained model checkpoints, logs, and other artifacts will be saved. It will also be the default name of the model when pushed to the hub if not redefined later.\n",
        "per_device_train_batch_size = 1\n",
        "per_device_eval_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "logging_steps = 100\n",
        "learning_rate = 1e-4 # The initial learning rate for the optimizer.\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "num_train_epochs=1\n",
        "warmup_ratio = 0.1\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_seq_length = 1500\n",
        "max_steps = 5000\n",
        "\n",
        "training_arguments = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    save_steps=500,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\",\n",
        "    bf16=True,\n",
        "    hub_private_repo=False,\n",
        "    push_to_hub=False,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    packing=True,\n",
        "    max_seq_length=max_seq_length,\n",
        "    max_steps = max_steps\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:59:25.695804Z",
          "iopub.execute_input": "2025-03-05T09:59:25.696087Z",
          "iopub.status.idle": "2025-03-05T09:59:25.724707Z",
          "shell.execute_reply.started": "2025-03-05T09:59:25.696065Z",
          "shell.execute_reply": "2025-03-05T09:59:25.724159Z"
        },
        "id": "8DUiqhMJDWYh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset[\"train\"].take(10000),\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T09:59:30.565610Z",
          "iopub.execute_input": "2025-03-05T09:59:30.565913Z",
          "iopub.status.idle": "2025-03-05T10:01:53.792051Z",
          "shell.execute_reply.started": "2025-03-05T09:59:30.565887Z",
          "shell.execute_reply": "2025-03-05T10:01:53.791353Z"
        },
        "colab": {
          "referenced_widgets": [
            "47d28fd39b09442f8c5c90883b76641a",
            "2c68592e248a487f995f803d3a4e2159",
            "9a3e6aa3113e44d0bd5b0d733366243a",
            "aae0f0aa7b46417d9ec3ef085a280616",
            "203db62c8e0f435ba6dc184263c991dd",
            "5a6d3e13b7694143b501cb96386e16ef",
            "09869c667caf4ffca76bfbc45395ab66",
            "d63d36029f984a23a5f13076efa3e931"
          ]
        },
        "id": "ixeBGaqpDWYh",
        "outputId": "144c1bc2-c790-4d50-e1b6-a34b1b1e6b10"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Converting train dataset to ChatML:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47d28fd39b09442f8c5c90883b76641a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Applying chat template to train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c68592e248a487f995f803d3a4e2159"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Tokenizing train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a3e6aa3113e44d0bd5b0d733366243a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Packing train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aae0f0aa7b46417d9ec3ef085a280616"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Converting eval dataset to ChatML:   0%|          | 0/18 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "203db62c8e0f435ba6dc184263c991dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Applying chat template to eval dataset:   0%|          | 0/18 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a6d3e13b7694143b501cb96386e16ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Tokenizing eval dataset:   0%|          | 0/18 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09869c667caf4ffca76bfbc45395ab66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Packing eval dataset:   0%|          | 0/18 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d63d36029f984a23a5f13076efa3e931"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "FKC08ujjDWYh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T11:09:12.373888Z",
          "iopub.execute_input": "2025-03-05T11:09:12.374172Z",
          "iopub.status.idle": "2025-03-05T11:09:12.830639Z",
          "shell.execute_reply.started": "2025-03-05T11:09:12.374150Z",
          "shell.execute_reply": "2025-03-05T11:09:12.830009Z"
        },
        "id": "ayPxvrclDWYh",
        "outputId": "6c10778c-0360-4cde-b6df-f386299d2bf0"
      },
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "139"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "trusted": true,
        "id": "kd9lj5idDWYh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(model.generate(tokenizer(\"\"\"Problem: Among the following given objects, those that can represent a set are ( )\\nA: All very large numbers\\nB: Numbers infinitely close to zero\\nC: Smart people\\nD: Real number roots of the equation $x^2 = 2$\\nReasoning: \"\"\",return_tensors='pt').to('cuda').input_ids)[0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T11:08:32.728208Z",
          "iopub.execute_input": "2025-03-05T11:08:32.728547Z",
          "iopub.status.idle": "2025-03-05T11:08:35.668519Z",
          "shell.execute_reply.started": "2025-03-05T11:08:32.728517Z",
          "shell.execute_reply": "2025-03-05T11:08:35.667934Z"
        },
        "id": "CSXthCKODWYh",
        "outputId": "1211d698-a860-48d9-aec9-bef62a93be06"
      },
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Problem: Among the following given objects, those that can represent a set are ( )\\nA: All very large numbers\\nB: Numbers infinitely close to zero\\nC: Smart people\\nD: Real number roots of the equation $x^2 = 2$\\nReasoning: <think>\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "raw",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "sQ14sd2qDWYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]['text']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T11:08:14.550475Z",
          "iopub.execute_input": "2025-03-05T11:08:14.550916Z",
          "iopub.status.idle": "2025-03-05T11:08:14.556286Z",
          "shell.execute_reply.started": "2025-03-05T11:08:14.550883Z",
          "shell.execute_reply": "2025-03-05T11:08:14.555534Z"
        },
        "id": "UOhEyuO1DWYi",
        "outputId": "ff79c99c-e873-40a0-a2f0-ba23893128d2"
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Problem: Among the following given objects, those that can represent a set are ( )\\nA: All very large numbers\\nB: Numbers infinitely close to zero\\nC: Smart people\\nD: Real number roots of the equation $x^2 = 2$\\nReasoning: <think>\\nOkay, so I have this multiple-choice question here about which of the given options can represent a set. The options are A, B, C, and D. Let me try to understand what each option is saying and whether they can be considered as sets.\\n\\nFirst, let\\'s recall what a set is in mathematics. A set is a well-defined collection of distinct objects, which are called elements or members of the set. The key thing here is that the definition of a set must be clear and unambiguous so that we can determine whether a particular object is or isn\\'t in the set.\\n\\nNow, let\\'s look at each option one by one.\\n\\n**Option A: All very large numbers**\\n\\nHmm, \"very large numbers.\" That sounds a bit vague. What exactly constitutes a \"very large number\"? Is it numbers greater than a million? A billion? Or maybe something else? The term \"very large\" is subjective and depends on context. Without a specific definition or criterion, it\\'s hard to say exactly which numbers belong to this set. For example, if I say 1000 is a very large number, someone else might disagree and say it\\'s not that large. So, this lack of a clear definition makes me think that \"all very large numbers\" might not be a well-defined set.\\n\\n**Option B: Numbers infinitely close to zero**\\n\\nOkay, \"numbers infinitely close to zero.\" This sounds like it\\'s referring to numbers that are approaching zero but never actually reaching it. In calculus, we talk about limits approaching zero, but in terms of sets, how do we define \"infinitely close\"? Is there a specific set of numbers that are infinitely close to zero? Or is this more of a conceptual idea rather than a concrete set? I\\'m not sure. It feels like this is more of a description of a limit rather than a defined set of numbers. So, similar to option A, this might not be a well-defined set.\\n\\n**Option C: Smart people**\\n\\nAlright, \"smart people.\" Now, this is definitely subjective. What makes someone smart? Intelligence can be measured in various ways—like IQ scores, academic achievements, problem-solving skills, etc. But even then, there\\'s no universal standard for what defines a \"smart person.\" Different cultures, educational systems, and individuals have different criteria. So, trying to define \"smart people\" as a set would be challenging because the membership criteria are not clear or consistent. This makes me think that this isn\\'t a well-defined set either.\\n\\n**Option D: Real number roots of the equation \\\\(x^2 = 2\\\\)**\\n\\nOkay, this one seems more concrete. The equation \\\\(x^2 = 2\\\\) is a quadratic equation, and we can solve it to find its roots. Let\\'s do that real quick. If \\\\(x^2 = 2\\\\), then taking the square root of both sides, we get \\\\(x = \\\\sqrt{2}\\\\) or \\\\(x = -\\\\sqrt{2}\\\\). So, the real number roots are \\\\(\\\\sqrt{2}\\\\) and \\\\(-\\\\sqrt{2}\\\\). These are specific numbers, and there\\'s no ambiguity about which numbers belong to this set. It\\'s clearly defined and anyone can verify whether a number is a root of this equation or not. Therefore, this seems like a well-defined set.\\n\\nSo, summarizing my thoughts:\\n\\n- **Option A**: Not well-defined because \"very large\" is subjective.\\n- **Option B**: Not well-defined because \"infinitely close\" is more of a conceptual idea rather than a specific set.\\n- **Option C**: Not well-defined because \"smart\" is subjective and varies based on different criteria.\\n- **Option D**: Well-defined because it refers to specific real numbers that are roots of the equation \\\\(x^2 = 2\\\\).\\n\\nTherefore, the correct answer should be **Option D**.\\n</think>\\n\\nThe correct answer is:\\n\\n**D: Real number roots of the equation \\\\(x^2 = 2\\\\)**\\n\\n**Explanation:**\\n\\nA set must be well-defined, meaning its elements are clearly and unambiguously specified. Among the options provided:\\n\\n- **A: All very large numbers** – The term \"very large\" is subjective and lacks a clear definition, making it unsuitable as a set.\\n- **B: Numbers infinitely close to zero** – This concept is more abstract and does not define a specific set of numbers.\\n- **C: Smart people** – The term \"smart\" is subjective and varies based on different criteria, so it cannot form a well-defined set.\\n- **D: Real number roots of the equation \\\\(x^2 = 2\\\\)** – This is a precise definition with specific elements (\\\\(\\\\sqrt{2}\\\\) and \\\\(-\\\\sqrt{2}\\\\)), making it a valid set.\\n\\nThus, only **Option D** meets the criteria of a well-defined set.\\nSolution: Option A represents \"All very large numbers\". The term \"very large\" is not well-defined as it is subjective and lacks a clear criterion for determining membership in the set.\\n\\nOption B represents \"Numbers infinitely close to zero\". The concept of \"infinitely close\" does not clearly define a specific set of numbers, as it suggests an approach to zero without ever reaching it. This does not constitute a well-defined set as the elements are not clearly identifiable.\\n\\nOption C represents \"Smart people\". The criterion \"smart\" is subjective and can vary based on context, culture, and individual interpretation. No clear measurement or threshold exists to universally determine who qualifies as \"smart\", making it impossible to clearly define this as a set.\\n\\nOption D represents \"Real number roots of the equation $x^2 = 2$\". This description is precise and refers to a specific and clear criterion: the real numbers that when squared equal 2. This set is well-defined as it includes the specific numbers $\\\\sqrt{2}$ and $-\\\\sqrt{2}$, which can be clearly identified and verified. Thus, this is a valid representation of a set.\\n\\nTherefore, the correct choice is:\\n\\n\\\\[\\\\boxed{D: \\\\text{Real number roots of the equation } x^2 = 2}\\\\]'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "DYJOjD3sDWYi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "DGfg7dmIDWYi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenization function\n",
        "# def tokenize_function(example):\n",
        "#     return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "# dataset = dataset.shuffle()\n",
        "# dataset = dataset.train_test_split(0.1)\n",
        "# subset  =dataset.take(10000)\n",
        "# tokenized_datasets = subset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KOye8RO8DWYi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T08:36:44.903155Z",
          "iopub.status.idle": "2025-02-16T08:36:44.903448Z",
          "shell.execute_reply": "2025-02-16T08:36:44.903309Z"
        },
        "id": "Zm3H-_PRDWYi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./qwen-sft\", per_device_train_batch_size=1, gradient_accumulation_steps=8,\n",
        "    eval_strategy=\"log\", save_strategy=\"epoch\", logging_steps=10,max_steps = 1000,\n",
        "    num_train_epochs=3, learning_rate=2e-5, weight_decay=0.01,\n",
        "    fp16=True, save_total_limit=2, report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model, args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    processing_class=tokenizer,\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T05:45:07.020827Z",
          "iopub.execute_input": "2025-02-16T05:45:07.021207Z",
          "iopub.status.idle": "2025-02-16T05:46:13.014886Z",
          "shell.execute_reply.started": "2025-02-16T05:45:07.021173Z",
          "shell.execute_reply": "2025-02-16T05:46:13.013980Z"
        },
        "colab": {
          "referenced_widgets": [
            "3400477bdbc34ab481fae2084790d3ce",
            "8a3a4a320275438889c656f7ec008cd1"
          ]
        },
        "id": "bOLGH2AmDWYi",
        "outputId": "4804a73c-58c6-4068-dce1-580cc2658330"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Tokenizing train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3400477bdbc34ab481fae2084790d3ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Tokenizing train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a3a4a320275438889c656f7ec008cd1"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(trainer.train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T05:39:54.424542Z",
          "iopub.execute_input": "2025-02-16T05:39:54.424833Z",
          "iopub.status.idle": "2025-02-16T05:39:54.433022Z",
          "shell.execute_reply.started": "2025-02-16T05:39:54.424811Z",
          "shell.execute_reply": "2025-02-16T05:39:54.432273Z"
        },
        "id": "Fd_bASf2DWYi",
        "outputId": "0e72cb97-813b-412e-e11c-9918e470b1ce"
      },
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Problem: There were 27 boys and 35 girls on the playground at recess. There were _____ children on the playground at recess.\\nReasoning: <think>\\nFirst, I need to determine the total number of children on the playground by adding the number of boys and girls.\\n\\nThere are 27 boys and 35 girls.\\n\\nAdding these together: 27 boys + 35 girls = 62 children.\\n\\nTherefore, the total number of children on the playground is 62.\\n</think>\\n\\nTo find the total number of children on the playground, we simply add the number of boys and girls together.\\n\\n\\\\[\\n\\\\text{Total children} = \\\\text{Number of boys} + \\\\text{Number of girls}\\n\\\\]\\n\\nPlugging in the given values:\\n\\n\\\\[\\n\\\\text{Total children} = 27 \\\\text{ boys} + 35 \\\\text{ girls} = 62 \\\\text{ children}\\n\\\\]\\n\\n**Final Answer:**\\n\\n\\\\[\\n\\\\boxed{62}\\n\\\\]\\nSolution: \\nThere were 62 children on the playground at recess. (27 boys + 35 girls = $\\\\boxed{62}$  children)'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T05:46:13.016143Z",
          "iopub.execute_input": "2025-02-16T05:46:13.016414Z",
          "iopub.status.idle": "2025-02-16T05:46:13.700628Z",
          "shell.execute_reply.started": "2025-02-16T05:46:13.016369Z",
          "shell.execute_reply": "2025-02-16T05:46:13.699875Z"
        },
        "id": "TmZba-67DWYi",
        "outputId": "464642e5-498b-469c-8896-e4fec062d1f8"
      },
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "152"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"]  =\"true\"\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T05:39:58.143099Z",
          "iopub.execute_input": "2025-02-16T05:39:58.143677Z",
          "iopub.status.idle": "2025-02-16T05:39:58.148015Z",
          "shell.execute_reply.started": "2025-02-16T05:39:58.143638Z",
          "shell.execute_reply": "2025-02-16T05:39:58.147146Z"
        },
        "id": "SH8_8T88DWYi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T07:00:05.331023Z",
          "iopub.execute_input": "2025-02-16T07:00:05.331259Z",
          "iopub.status.idle": "2025-02-16T07:41:23.853606Z",
          "shell.execute_reply.started": "2025-02-16T07:00:05.331237Z",
          "shell.execute_reply": "2025-02-16T07:41:23.852297Z"
        },
        "id": "3KcV1qwjDWYi",
        "outputId": "c9cae6ff-94c7-4bff-d9f4-0e226395b836"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='356' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 356/1000 41:07 < 1:14:48, 0.14 it/s, Epoch 0.28/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.359000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.018400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.277800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.204200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.253900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.324900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>3.337100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.457900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>3.216800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.515600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>3.275400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>3.301700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>3.502500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>3.431000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>3.162200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>3.175700</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>3.251500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>3.315700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.993500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.369600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.950000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>3.101200</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>3.059100</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.988900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>3.119800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>3.355300</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>3.148100</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.945700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>3.037100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.250800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>3.303100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>3.151700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>3.236000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>3.000400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>3.158700</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-635967aca760>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3686\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3689\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2242\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2245\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell only runs in our colab environment as I have added my huggingface token inside the environment key (HF_TOKEN)\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T07:42:14.045580Z",
          "iopub.execute_input": "2025-02-16T07:42:14.045913Z",
          "iopub.status.idle": "2025-02-16T07:42:17.998755Z",
          "shell.execute_reply.started": "2025-02-16T07:42:14.045883Z",
          "shell.execute_reply": "2025-02-16T07:42:17.997576Z"
        },
        "colab": {
          "referenced_widgets": [
            "2f100626a43c4a5a9d1d1a1352716d47",
            "96b730171d7d4d1ea43c6ed38d668989",
            "15f1d10d23cc451da1d7bee427f7b97c",
            "1740db3261ae423e88083b0d9de3b42a"
          ]
        },
        "id": "-DySzSJLDWYi",
        "outputId": "0aee8b80-7360-4b6a-8a53-56c17f534be1"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f100626a43c4a5a9d1d1a1352716d47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/7.39M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96b730171d7d4d1ea43c6ed38d668989"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15f1d10d23cc451da1d7bee427f7b97c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1740db3261ae423e88083b0d9de3b42a"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/Sujithanumala/qwen-sft/commit/63133390c8df3f35a2f6907312144abeead52db5', commit_message='Qwen3B_ThinkingModel', commit_description='', oid='63133390c8df3f35a2f6907312144abeead52db5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Sujithanumala/qwen-sft', endpoint='https://huggingface.co', repo_type='model', repo_id='Sujithanumala/qwen-sft'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "text= dataset['train'][1000]['problem']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T07:47:55.094619Z",
          "iopub.execute_input": "2025-02-16T07:47:55.094929Z",
          "iopub.status.idle": "2025-02-16T07:47:55.099081Z",
          "shell.execute_reply.started": "2025-02-16T07:47:55.094907Z",
          "shell.execute_reply": "2025-02-16T07:47:55.098189Z"
        },
        "id": "RW86qD7FDWYi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.decode(model.generate(tokenizer(text,return_tensors = 'pt').to('cuda')['input_ids'],max_new_tokens=500)[0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T07:47:55.797845Z",
          "iopub.execute_input": "2025-02-16T07:47:55.798099Z",
          "iopub.status.idle": "2025-02-16T07:48:21.414419Z",
          "shell.execute_reply.started": "2025-02-16T07:47:55.798079Z",
          "shell.execute_reply": "2025-02-16T07:48:21.413692Z"
        },
        "id": "25Em3KhfDWYj",
        "outputId": "25a5f17e-d645-4372-f636-ca021b678c3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T07:48:21.415385Z",
          "iopub.execute_input": "2025-02-16T07:48:21.415655Z",
          "iopub.status.idle": "2025-02-16T07:48:21.419962Z",
          "shell.execute_reply.started": "2025-02-16T07:48:21.415632Z",
          "shell.execute_reply": "2025-02-16T07:48:21.419270Z"
        },
        "id": "zszhf-1YDWYj",
        "outputId": "6d9b5682-840b-4393-f62f-f42980738971"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Calculate: $3x^{2}y\\cdot \\left(-2xy\\right)^{2}=\\_\\_\\_\\_\\_\\_$. To solve the expression \\(3x^{2}y \\cdot \\left(-2xy\\right)^{2}\\), we will follow these steps:\n\n1. Simplify the expression inside the parentheses.\n2. Apply the exponent to the simplified expression.\n3. Multiply the resulting expression by \\(3x^{2}y\\).\n\nLet's start with the expression inside the parentheses: \\(\\left(-2xy\\right)^{2}\\). When we square a product, we square each factor individually. So, we have:\n\\[\n\\left(-2xy\\right)^{2} = (-2)^2 \\cdot x^2 \\cdot y^2 = 4x^2y^2.\n\\]\nNow, substitute this back into the original expression:\n\\[\n3x^{2}y \\cdot 4x^{2}y^{2}.\n\\]\nNext, we multiply the coefficients and the variables separately. The coefficients are 3 and 4, so their product is:\n\\[\n3 \\cdot 4 = 12.\n\\]\nFor the variables, we use the property of exponents that states \\(a^m \\cdot a^n = a^{m+n}\\). So, we have:\n\\[\nx^2 \\cdot x^2 = x^{2+2} = x^4,\n\\]\nand\n\\[\ny \\cdot y^2 = y^{1+2} = y^3.\n\\]\nPutting it all together, we get:\n\\[\n12x^4y^3.\n\\]\nTherefore, the final answer is:\n\\[\n\\boxed{12x^4y^3}.\n\\]<|endoftext|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# actual_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "m9BaP9yZDWYj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_output = tokenizer.decode(actual_model.generate(tokenizer(text,return_tensors = 'pt').to('cuda')['input_ids'],max_new_tokens=500)[0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T07:48:21.421509Z",
          "iopub.execute_input": "2025-02-16T07:48:21.421728Z",
          "iopub.status.idle": "2025-02-16T07:48:37.947576Z",
          "shell.execute_reply.started": "2025-02-16T07:48:21.421700Z",
          "shell.execute_reply": "2025-02-16T07:48:37.946643Z"
        },
        "id": "t_0-KOF3DWYj",
        "outputId": "f8727dd5-2863-4f42-9317-19eee2fc741f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_output)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T07:48:37.948542Z",
          "iopub.execute_input": "2025-02-16T07:48:37.948790Z",
          "iopub.status.idle": "2025-02-16T07:48:37.953186Z",
          "shell.execute_reply.started": "2025-02-16T07:48:37.948768Z",
          "shell.execute_reply": "2025-02-16T07:48:37.952344Z"
        },
        "id": "_bME0PiwDWYj",
        "outputId": "4d5fdbe0-ccf0-4ffa-ba45-c006bbe121f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Calculate: $3x^{2}y\\cdot \\left(-2xy\\right)^{2}=\\_\\_\\_\\_\\_\\_$. To solve the expression \\(3x^2y \\cdot (-2xy)^2\\), we will follow these steps:\n\n1. Simplify the expression inside the parentheses.\n2. Apply the exponent to the simplified expression.\n3. Multiply the resulting expression by \\(3x^2y\\).\n\nLet's start with the expression inside the parentheses:\n\n\\[\n(-2xy)^2\n\\]\n\nWhen we square a product, we square each factor:\n\n\\[\n(-2xy)^2 = (-2)^2 \\cdot x^2 \\cdot y^2 = 4x^2y^2\n\\]\n\nNow we substitute this back into the original expression:\n\n\\[\n3x^2y \\cdot 4x^2y^2\n\\]\n\nNext, we multiply the coefficients and the variables separately. The coefficients are 3 and 4, and the variables are \\(x^2\\) and \\(y^2\\):\n\n\\[\n3 \\cdot 4 = 12\n\\]\n\\[\nx^2 \\cdot x^2 = x^{2+2} = x^4\n\\]\n\\[\ny \\cdot y^2 = y^{1+2} = y^3\n\\]\n\nPutting it all together, we get:\n\n\\[\n12x^4y^3\n\\]\n\nSo the final answer is:\n\n\\[\n\\boxed{12x^4y^3}\n\\]<|endoftext|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "output==base_output"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T07:46:27.290817Z",
          "iopub.execute_input": "2025-02-16T07:46:27.291115Z",
          "iopub.status.idle": "2025-02-16T07:46:27.296091Z",
          "shell.execute_reply.started": "2025-02-16T07:46:27.291092Z",
          "shell.execute_reply": "2025-02-16T07:46:27.295348Z"
        },
        "id": "dp6AHg_4DWYj",
        "outputId": "e624a6de-9910-43dd-beb2-52312afaebd9"
      },
      "outputs": [
        {
          "execution_count": 41,
          "output_type": "execute_result",
          "data": {
            "text/plain": "False"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "KnIoqTq2DWYj"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}